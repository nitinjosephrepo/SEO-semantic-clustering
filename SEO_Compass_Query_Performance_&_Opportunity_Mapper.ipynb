{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. INSTALLS & IMPORTS"
      ],
      "metadata": {
        "id": "scLPjU95ZX51"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k63fneQ9QFgS"
      },
      "outputs": [],
      "source": [
        "# Install libraries\n",
        "# !pip install -q sentence-transformers scikit-learn umap-learn seaborn matplotlib pandas numpy\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ML & NLP Libraries\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Dimensionality Reduction\n",
        "try:\n",
        "    import umap.umap_ as umap\n",
        "except ImportError:\n",
        "    print(\"UMAP not available. Install 'umap-learn' to use UMAP visualizations.\")\n",
        "    umap = None\n",
        "\n",
        "# Plotting Style\n",
        "sns.set_theme(style=\"whitegrid\", context=\"talk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. CONFIGURATION"
      ],
      "metadata": {
        "id": "774vqfNWaaPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use all-mpnet-base-v2 which performs well on short texts like keywords and for generating high-quality semantic embeddings.\n",
        "# Clustering Logic: Sets the target number of topics (n_clusters=15) and fixes the random seed (42) so results remain consistent across runs.\n",
        "# Data Cleaning: Establishes thresholds (minimum impressions) and regex patterns to filter out noise and brand-specific terms before processing.\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # --- Clustering Settings ---\n",
        "    # Enhanced model for short text\n",
        "    embedding_model: str = \"all-mpnet-base-v2\"\n",
        "    n_clusters: int = 15\n",
        "    random_state: int = 42\n",
        "\n",
        "    # --- Filtering ---\n",
        "    min_impressions: int = 10  # Lowered for mock data example\n",
        "    brand_regex: str = r\"\"\n",
        "    exclude_regex: str = r\"\"\n",
        "\n",
        "CFG = Config()"
      ],
      "metadata": {
        "id": "qmFQe08sQihF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Data Loading & Processing"
      ],
      "metadata": {
        "id": "UdGoJ29eahSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate_mock_data: Create synthetic \"Before\" and \"After\" SEO datasets centered around specific\n",
        "#topics (e.g., cremation, urns) to test the pipeline without needing a live API connection.\n",
        "# data_a = previous period vs data_b = latest period\n",
        "\n",
        "def generate_mock_data(n_rows=500):\n",
        "    \"\"\"Generates synthetic SEO data for testing.\"\"\"\n",
        "    # Seed topics for clustering coherence\n",
        "    topics = [\"cremation cost\", \"pet urns\", \"funeral services\", \"biodegradable caskets\", \"cremation jewelry\"]\n",
        "    data_a = []\n",
        "    data_b = []\n",
        "\n",
        "    for i in range(n_rows):\n",
        "        topic = np.random.choice(topics)\n",
        "        suffix = np.random.choice([\"near me\", \"prices\", \"for dogs\", \"online\", \"reviews\", \"in california\"])\n",
        "        query = f\"{topic} {suffix} {i}\"\n",
        "\n",
        "        # Period A Metrics\n",
        "        imps_a = np.random.randint(100, 5000)\n",
        "        clicks_a = int(imps_a * np.random.uniform(0.01, 0.15))\n",
        "        pos_a = np.random.uniform(1.0, 60.0)\n",
        "\n",
        "        # Period B Metrics (with random drift)\n",
        "        imps_b = int(imps_a * np.random.uniform(0.8, 1.5))\n",
        "        clicks_b = int(imps_b * np.random.uniform(0.01, 0.15))\n",
        "        pos_b = pos_a + np.random.uniform(-5, 5)\n",
        "        pos_b = max(1.0, pos_b)\n",
        "\n",
        "        data_a.append({\"query\": query, \"clicks\": clicks_a, \"impressions\": imps_a, \"ctr\": clicks_a/imps_a if imps_a > 0 else 0, \"position\": pos_a})\n",
        "        data_b.append({\"query\": query, \"clicks\": clicks_b, \"impressions\": imps_b, \"ctr\": clicks_b/imps_b if imps_b > 0 else 0, \"position\": pos_b})\n",
        "\n",
        "    return pd.DataFrame(data_a), pd.DataFrame(data_b)\n",
        "\n",
        "def clean_filter_queries(df: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
        "    \"\"\"Filters data based on regex and minimum impressions.\"\"\"\n",
        "    if df.empty: return df\n",
        "    out = df.copy()\n",
        "    if \"query\" in out.columns:\n",
        "        out[\"query\"] = out[\"query\"].astype(str)\n",
        "        if cfg.brand_regex:\n",
        "            out = out[out[\"query\"].str.contains(cfg.brand_regex, flags=re.I, regex=True, na=False)]\n",
        "        if cfg.exclude_regex:\n",
        "            out = out[~out[\"query\"].str.contains(cfg.exclude_regex, flags=re.I, regex=True, na=False)]\n",
        "    if \"impressions\" in out.columns:\n",
        "        out = out[out[\"impressions\"] >= cfg.min_impressions].copy()\n",
        "    return out\n",
        "\n",
        "def weighted_mean(values, weights):\n",
        "    if sum(weights) == 0: return 0\n",
        "    return np.average(values, weights=weights)\n",
        "\n",
        "def aggregate_by_query(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Aggregates metrics by query.\"\"\"\n",
        "    grp_cols = [c for c in df.columns if c in (\"period\", \"query\")]\n",
        "    agg = (df.groupby(grp_cols, as_index=False)\n",
        "             .agg(clicks=(\"clicks\", \"sum\"),\n",
        "                  impressions=(\"impressions\", \"sum\"),\n",
        "                  position=(\"position\", lambda x: weighted_mean(x, df.loc[x.index, \"impressions\"]))))\n",
        "    agg[\"ctr\"] = np.where(agg[\"impressions\"] > 0, agg[\"clicks\"] / agg[\"impressions\"], 0.0)\n",
        "    return agg"
      ],
      "metadata": {
        "id": "X5U2plAkRKdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. CLUSTERING LOGIC"
      ],
      "metadata": {
        "id": "qVDi3jUsUf9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform semantic clustering & topic extraction: This block performs the core machine learning tasks to group search queries by intent rather than just keywords.\n",
        "# Function iterates through data to determine the optimal number of clusters to optimize for silhouette score\n",
        "# Performs vectorize and grouping: converts raw text into semantic vectors using K-means algorithm\n",
        "# Output is clustered data with labels\n",
        "\n",
        "def pick_k_auto(embeddings: np.ndarray, k_min=3, k_max=20, random_state=42):\n",
        "    best = (None, -1)\n",
        "    k_max = min(k_max, len(embeddings) - 1)\n",
        "    if k_max <= k_min: return k_min, -1\n",
        "\n",
        "    print(\"Auto-tuning cluster count (k)...\")\n",
        "    for k in range(k_min, k_max):\n",
        "        km = KMeans(n_clusters=k, random_state=random_state, n_init=\"auto\")\n",
        "        labels = km.fit_predict(embeddings)\n",
        "        try:\n",
        "            score = silhouette_score(embeddings, labels)\n",
        "            if score > best[1]: best = (k, score)\n",
        "        except: continue\n",
        "    return best\n",
        "\n",
        "def cluster_queries(df_agg: pd.DataFrame, cfg: Config):\n",
        "    queries = df_agg[\"query\"].unique().tolist()\n",
        "    if len(queries) < 5:\n",
        "        print(\"Not enough queries to perform clustering.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    print(f\"Embedding {len(queries)} unique queries using {cfg.embedding_model}...\")\n",
        "    model = SentenceTransformer(cfg.embedding_model)\n",
        "    emb = model.encode(queries, normalize_embeddings=True)\n",
        "\n",
        "    if cfg.n_clusters is None:\n",
        "        k, s = pick_k_auto(emb, random_state=cfg.random_state)\n",
        "        print(f\"Auto-picked k={k} (Silhouette Score={s:.3f})\")\n",
        "    else:\n",
        "        k = min(cfg.n_clusters, len(queries)-1)\n",
        "        print(f\"Using fixed k={k}\")\n",
        "\n",
        "    km = KMeans(n_clusters=k, random_state=cfg.random_state, n_init=\"auto\")\n",
        "    labels = km.fit_predict(emb)\n",
        "\n",
        "    q2c = dict(zip(queries, labels))\n",
        "    out = df_agg.copy()\n",
        "    out[\"cluster_id\"] = out[\"query\"].map(q2c)\n",
        "\n",
        "    # --- Centroid Naming ---\n",
        "    print(\"Naming clusters via Centroid query...\")\n",
        "    cluster_names = {}\n",
        "    for cluster_id in range(k):\n",
        "        cluster_indices = np.where(labels == cluster_id)[0]\n",
        "        if len(cluster_indices) > 0:\n",
        "            cluster_embs = emb[cluster_indices]\n",
        "            center = km.cluster_centers_[cluster_id]\n",
        "            sims = np.dot(cluster_embs, center)\n",
        "            best_idx = np.argmax(sims)\n",
        "            cluster_names[cluster_id] = queries[cluster_indices[best_idx]].upper()\n",
        "        else:\n",
        "            cluster_names[cluster_id] = f\"CLUSTER_{cluster_id}\"\n",
        "\n",
        "    out[\"cluster_name\"] = out[\"cluster_id\"].map(cluster_names)\n",
        "    return out, km, emb, model"
      ],
      "metadata": {
        "id": "w47a1WDvRTrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. METRICS LOGIC\n"
      ],
      "metadata": {
        "id": "0oizRNcXUnYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def cluster_period_metrics(df_clustered: pd.DataFrame) -> pd.DataFrame:\n",
        "    m = (df_clustered.groupby([\"cluster_id\", \"cluster_name\", \"period\"], as_index=False)\n",
        "            .agg(clicks=(\"clicks\", \"sum\"),\n",
        "                 impressions=(\"impressions\", \"sum\"),\n",
        "                 position=(\"position\", lambda x: weighted_mean(x, df_clustered.loc[x.index, \"impressions\"]))))\n",
        "    m[\"ctr\"] = np.where(m[\"impressions\"] > 0, m[\"clicks\"] / m[\"impressions\"], 0.0)\n",
        "    return m\n",
        "\n",
        "def pivot_two_periods(cluster_m: pd.DataFrame) -> pd.DataFrame:\n",
        "    p = cluster_m.pivot_table(\n",
        "        index=[\"cluster_id\", \"cluster_name\"],\n",
        "        columns=\"period\",\n",
        "        values=[\"clicks\", \"impressions\", \"ctr\", \"position\"],\n",
        "        aggfunc=\"first\"\n",
        "    )\n",
        "    p.columns = [f\"{metric}_{period}\" for metric, period in p.columns]\n",
        "    p = p.reset_index()\n",
        "    p.fillna(0, inplace=True)\n",
        "\n",
        "    for metric in [\"clicks\", \"impressions\", \"ctr\", \"position\"]:\n",
        "        if f\"{metric}_A\" in p.columns and f\"{metric}_B\" in p.columns:\n",
        "            p[f\"delta_{metric}\"] = p[f\"{metric}_B\"] - p[f\"{metric}_A\"]\n",
        "\n",
        "    # --- Scores ---\n",
        "    p[\"opportunity_score\"] = np.where(\n",
        "        (p[\"delta_clicks\"] > 0) & (p[\"delta_position\"] < 0),\n",
        "        p[\"delta_clicks\"] + np.abs(p[\"delta_position\"] * 10), 0\n",
        "    )\n",
        "    p[\"risk_score\"] = np.where(\n",
        "        (p[\"delta_clicks\"] < 0) & (p[\"delta_position\"] > 0),\n",
        "        np.abs(p[\"delta_clicks\"]) + (p[\"delta_position\"] * 10), 0\n",
        "    )\n",
        "\n",
        "    clicks_scale = np.nanmedian(np.abs(p[\"delta_clicks\"])) + 1e-9\n",
        "    pos_scale = np.nanmedian(np.abs(p[\"delta_position\"])) + 1e-9\n",
        "    p[\"total_change_score\"] = (np.abs(p[\"delta_clicks\"]) / clicks_scale) + (np.abs(p[\"delta_position\"]) / pos_scale)\n",
        "\n",
        "    return p.sort_values(\"total_change_score\", ascending=False)"
      ],
      "metadata": {
        "id": "jZwqNfqxRVDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. VIZ FUNCTIONS\n"
      ],
      "metadata": {
        "id": "78I5dv0vU6FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_umap_clusters(embeddings: np.ndarray, labels: np.ndarray, cluster_names: dict, title: str, figsize=(14, 10), legend_loc=(1.05, 1), legend_fontsize=10, tick_fontsize=10):\n",
        "    if umap is None: return\n",
        "    print(\"Generating UMAP Projection...\")\n",
        "    scaled_data = StandardScaler().fit_transform(embeddings)\n",
        "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, metric='cosine', random_state=42)\n",
        "    embedding_2d = reducer.fit_transform(scaled_data)\n",
        "\n",
        "    plot_df = pd.DataFrame(embedding_2d, columns=['UMAP_1', 'UMAP_2'])\n",
        "    plot_df['Cluster_ID'] = labels\n",
        "    plot_df['Cluster_Name'] = plot_df['Cluster_ID'].map(cluster_names)\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.scatterplot(x='UMAP_1', y='UMAP_2', hue='Cluster_Name', data=plot_df, palette=sns.color_palette(\"muted\"), alpha=0.8, s=80)\n",
        "    plt.title(title, fontsize=12)\n",
        "    plt.legend(bbox_to_anchor=legend_loc, loc=2, borderaxespad=0., fontsize=legend_fontsize)\n",
        "    plt.tick_params(axis='both', which='major', labelsize=tick_fontsize)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_delta_quadrant(pivoted: pd.DataFrame, title: str, text_fontsize=9, xlabel_fontsize=12, ylabel_fontsize=12):\n",
        "    data = pivoted.copy()\n",
        "    # Check if we have B data to plot, if not, fill with 0\n",
        "    if 'impressions_B' not in data.columns: data['impressions_B'] = 0\n",
        "    if 'impressions_A' not in data.columns: data['impressions_A'] = 0\n",
        "    data['total_impressions'] = data['impressions_A'] + data['impressions_B']\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.scatterplot(data=data, x=\"delta_position\", y=\"delta_clicks\", size=\"total_impressions\",\n",
        "                    sizes=(100, 2000), hue=\"cluster_name\", legend=False, alpha=0.7, palette=\"tab10\")\n",
        "\n",
        "    plt.axvline(0, color='gray', linestyle='--', linewidth=1)\n",
        "    plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
        "    plt.text(data[\"delta_position\"].min()*0.9, data[\"delta_clicks\"].max()*0.9, \"OPPORTUNITY\\n(Rank Up, Clicks Up)\", color='green', weight='bold', fontsize=text_fontsize+2)\n",
        "    plt.text(data[\"delta_position\"].max()*0.9, data[\"delta_clicks\"].min()*0.9, \"RISK\\n(Rank Down, Clicks Down)\", color='red', weight='bold', ha='right', fontsize=text_fontsize+2)\n",
        "\n",
        "    top_movers = data.sort_values(\"total_change_score\", ascending=False).head(5)\n",
        "    for i, row in top_movers.iterrows():\n",
        "        plt.text(row[\"delta_position\"], row[\"delta_clicks\"], row[\"cluster_name\"], fontsize=text_fontsize, weight='bold')\n",
        "\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(\"Delta Position (Negative = Improvement)\", fontsize=xlabel_fontsize)\n",
        "    plt.ylabel(\"Delta Clicks (Positive = Growth)\", fontsize=ylabel_fontsize)\n",
        "    plt.gca().invert_xaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_trajectory(cluster_m: pd.DataFrame, title: str, text_fontsize=9, xlabel_fontsize=8, ylabel_fontsize=8):\n",
        "    periods = sorted(cluster_m[\"period\"].unique())\n",
        "    if len(periods) != 2: return\n",
        "    A, B = periods\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "    sns.scatterplot(data=cluster_m, x=\"position\", y=\"clicks\", size=\"impressions\",\n",
        "                    hue=\"cluster_name\", style=\"period\", markers={\"A\": \"o\", \"B\": \"X\"},\n",
        "                    sizes=(100, 1500), alpha=0.7, ax=ax, legend=False, palette=\"tab10\")\n",
        "\n",
        "    A_df = cluster_m[cluster_m[\"period\"] == A].set_index(\"cluster_id\")\n",
        "    B_df = cluster_m[cluster_m[\"period\"] == B].set_index(\"cluster_id\")\n",
        "    joined = A_df[[\"cluster_name\",\"clicks\",\"position\"]].join(B_df[[\"clicks\",\"position\"]], lsuffix=f\"_{A}\", rsuffix=f\"_{B}\")\n",
        "    palette = sns.color_palette(\"tab10\", n_colors=len(joined))\n",
        "\n",
        "    for i, (idx, r) in enumerate(joined.iterrows()):\n",
        "        plt.plot([r[f\"position_{A}\"], r[f\"position_{B}\"]], [r[f\"clicks_{A}\"], r[f\"clicks_{B}\"]], color=palette[i%len(palette)], linestyle='--', linewidth=1.5, alpha=0.6)\n",
        "        ax.text(r[f\"position_{B}\"], r[f\"clicks_{B}\"], r[\"cluster_name\"], fontsize=text_fontsize, weight=\"bold\", color=palette[i%len(palette)])\n",
        "\n",
        "    ax.invert_xaxis()\n",
        "    ax.set_title(title, fontsize=14)\n",
        "    ax.set_xlabel(\"Average Position (Lower is Better)\", fontsize=xlabel_fontsize)\n",
        "    ax.set_ylabel(\"Total Clicks\", fontsize=ylabel_fontsize)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "f-N6hmsrRYND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup & Data Loading\n"
      ],
      "metadata": {
        "id": "UxQzZnVoVqO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 1: LOAD DATA ---\n",
        "# Toggle this boolean to switch between Synthetic and Real Data\n",
        "USE_SYNTHETIC_DATA = True\n",
        "\n",
        "if USE_SYNTHETIC_DATA:\n",
        "    print(\"Generating synthetic SEO data...\")\n",
        "    df_a_raw, df_b_raw = generate_mock_data(n_rows=500)\n",
        "else:\n",
        "    # --- OPTION: Load your own CSVs ---\n",
        "    # Ensure your CSVs have columns: query, clicks, impressions, ctr, position\n",
        "    print(\"Loading real data from CSV...\")\n",
        "    # df_a_raw = pd.read_csv(\"your_period_a.csv\")\n",
        "    # df_b_raw = pd.read_csv(\"your_period_b.csv\")\n",
        "\n",
        "    # Placeholder error to prevent crash if you run without uploading files\n",
        "    if 'df_a_raw' not in locals():\n",
        "        print(\"Error: CSV files not found. Reverting to synthetic data.\")\n",
        "        df_a_raw, df_b_raw = generate_mock_data(n_rows=500)\n",
        "\n",
        "# Process Data\n",
        "df_a = df_a_raw.copy(); df_a[\"period\"] = \"A\"\n",
        "df_b = df_b_raw.copy(); df_b[\"period\"] = \"B\"\n",
        "df_all = pd.concat([df_a, df_b], ignore_index=True)\n",
        "df_all = clean_filter_queries(df_all, CFG)\n",
        "df_agg = aggregate_by_query(df_all)\n",
        "\n",
        "print(f\"Data Loaded. Total unique queries: {df_agg['query'].nunique()}\")"
      ],
      "metadata": {
        "id": "WoaYHx-uRcd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 2: CLUSTER & ANALYZE ---\n",
        "df_clustered, kmeans, embeddings, model = cluster_queries(df_agg, CFG)\n",
        "\n",
        "if df_clustered is not None:\n",
        "    cluster_m = cluster_period_metrics(df_clustered)\n",
        "    pivoted = pivot_two_periods(cluster_m)\n",
        "\n",
        "    # Display Top Movers Table\n",
        "    print(\"\\n--- Top Clusters by Total Change ---\")\n",
        "    display(pivoted.head(10))\n",
        "\n",
        "    # Create map for visualizations\n",
        "    cluster_names_map = {id: name for id, name in pivoted.set_index('cluster_id')['cluster_name'].to_dict().items()}\n",
        "else:\n",
        "    print(\"Clustering failed. Check data.\")"
      ],
      "metadata": {
        "id": "_9puWY_lRfDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual 1: Semantic Map\n",
        "if df_clustered is not None:\n",
        "    plot_umap_clusters(embeddings, kmeans.labels_, cluster_names_map, f\"Semantic Query Clusters (Model: {CFG.embedding_model})\", legend_fontsize=8, tick_fontsize=8)"
      ],
      "metadata": {
        "id": "ftL_d2iDSgAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual 2: Opportunity vs Risk Quadrant\n",
        "if df_clustered is not None:\n",
        "    plot_delta_quadrant(pivoted, \"SEO Performance Quadrant\", text_fontsize=7, xlabel_fontsize=6, ylabel_fontsize=6)"
      ],
      "metadata": {
        "id": "5g5OlOrLSxW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual 3: Movement Trajectory (A -> B)\n",
        "if df_clustered is not None:\n",
        "    plot_trajectory(cluster_m, \"Cluster Movement: Period A vs B\", text_fontsize=7, xlabel_fontsize=10, ylabel_fontsize=10)"
      ],
      "metadata": {
        "id": "tIxNADimRjsL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}